{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80873bec-8d3a-4b41-96b3-fd13a1ccc1f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This solution accelerator notebook is available at [Databricks Industry Solutions](https://github.com/databricks-industry-solutions/manufacturing-root-cause-analysis/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93e9d4fb-85e5-46f1-b9af-94270ae733ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Answer causal questions\n",
    "We have reached the climax of this project! The ultimate goal of this use case was to uncover the true root causes behind a drop in quality in a specific product or a batch of products. In this notebook, we will:\n",
    "\n",
    "1. Conduct root cause analysis on a few specific products flagged as defective for various reasons.\n",
    "2. Perform root cause analysis on a batch of products exhibiting a higher defect rate compared to the training dataset.\n",
    "\n",
    "Refer to the notebook `01_causal_graph` for the recommended cluster configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "070bb616-b7d2-4de7-bc1a-2adb7c1427f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f91f9f5-2c5d-4051-8d5d-2fb0621ea210",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install graphviz from nicer visualization"
    }
   },
   "outputs": [],
   "source": [
    "%sh apt-get update && apt-get install -y graphviz graphviz-dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6981dab3-d692-4090-8df7-6892e828bf6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We install the required packages from the `requirements.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b0b4188-5fb2-4058-a3ba-41810d02d424",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install requirements"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -r ./requirements.txt --quiet\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ad5d409-c5b3-44f3-a838-159763a629d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In the next cell, we run the `99_utils` notebook, which defines a few utility functions that we will use along the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68760f8e-31d7-4c51-904b-e93c49257862",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Run utils notebook"
    }
   },
   "outputs": [],
   "source": [
    "%run ./99_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eac34b98-607f-40cb-82e0-e05e75db1e60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Define variables and set MLflow experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7965a137-7cfa-4006-bdee-0a43c47dd0bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow import MlflowClient\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dowhy import gcm\n",
    "from dowhy.utils import bar_plot\n",
    "\n",
    "mlflow.autolog(disable=True)  # Disabling MLflow autolog as we are just trying to optimise the process and don't need to log everything\n",
    "gcm.config.disable_progress_bars()  # We turn off the progress bars here to reduce the number of outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32f00afc-4846-4336-b34d-ac2351681022",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "user_name = spark.sql(\"SELECT current_user()\").collect()[0][0]\n",
    "first_name = user_name.split(\".\")[0]\n",
    "\n",
    "# Set up Unity Catalog\n",
    "catalog = f'causal_solacc_{first_name}'     # Change this to your catalog name\n",
    "schema = f'rca'                             # Change this to your schema name\n",
    "model = f\"manufacturing_rca\"                # Change this to your model name\n",
    "\n",
    "setup_unity_catalog(catalog, schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59493254-0b1f-4287-a32a-7ae66c894557",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load the causal model\n",
    "\n",
    "Let's load the fitted causal model from the previous notebook. We will use the \"champion\" alias to ensure the correct version is loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "337fdbfa-a0db-4456-aa24-e1ccf6070736",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "mlflow_client = MlflowClient()\n",
    "\n",
    "registered_model_name = f\"{catalog}.{schema}.{model}\"\n",
    "model = f\"models:/{registered_model_name}@champion\"\n",
    "\n",
    "# Load model as a PyFuncModel\n",
    "loaded_model = mlflow.pyfunc.load_model(model)\n",
    "loaded_scm = loaded_model.unwrap_python_model().load_scm()\n",
    "loaded_causal_graph = loaded_model.unwrap_python_model().load_causal_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b138129f-50f1-47c3-a20a-e252a742e174",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98aca0e8-d5fb-4c69-99c1-074a05f3cbac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train = spark.read.table(f\"{catalog}.{schema}.data_manufacturing\")\n",
    "train = train.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5e1a892-8d93-4508-af9d-c458f650af97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Conduct root cause analysis\n",
    "\n",
    "As mentioned earlier, we will explore two scenarios for causal investigation:\n",
    "\n",
    "1. Identifying the root causes of specific defective samples.\n",
    "2. Determining the root causes of a batch of samples with a high defect rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a57d4f8-e40c-415e-bc56-dcb0182b9a53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. What are the key factors causing the quality drop of a particular product?\n",
    "\n",
    "Performing this analysis requires new samples of defective products. For this, we will use the method [`gcm.draw_samples`](https://www.pywhy.org/dowhy/v0.9.1/user_guide/gcm_based_inference/draw_samples.html), which enables us to generate new samples using the fitted causal graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ce85e71-1889-4824-8f05-f213abdb8d9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "new_batch = gcm.draw_samples(loaded_scm, num_samples=100)\n",
    "new_batch.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "107f2e05-131e-4c18-bacb-f660f47ce1d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Above we generated 100 samples that consist of multiple defectove products (`quality = 1`). These originate from instances where either `dimension = 1`, `torque_checks = 1`, or `visual_inspection = 1`. To pinpoint the factors contributing to the quality drop, we use DoWhy's [confidence intervals feature](https://www.pywhy.org/dowhy/v0.9.1/user_guide/gcm_based_inference/estimating_confidence_intervals.html). This feature requires specifying the target node of interest (`quality`) and the anomaly sample to analyze."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4c20fb0-8121-4d5b-aa06-4aefa1eb205d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In the first example, we will see the root causes of a product that failed the **dimensional verification**. For this, we select one defective sample from the generated batch that did not pass the dimensional verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bac1836f-98d7-4947-b53e-83995c6cb794",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "defect_dimensions = new_batch[new_batch['dimensions'] == 1].iloc[0]\n",
    "display(defect_dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ed95a58-7c45-4833-8ab5-eff0b30e4bca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Compute confidence intervals for anomaly attributions\n",
    "median_attributions, confidence_intervals = gcm.confidence_intervals(\n",
    "    gcm.fit_and_compute(\n",
    "        gcm.attribute_anomalies,  # Function to attribute anomalies\n",
    "        loaded_scm,  # Structural causal model\n",
    "        bootstrap_training_data=train,  # Training data for bootstrapping\n",
    "        target_node='quality',  # Target node for anomaly attribution\n",
    "        anomaly_samples=pd.DataFrame([defect_dimensions])  # Anomaly samples to analyze\n",
    "    ),\n",
    "    num_bootstrap_resamples=10  # Number of bootstrap resamples\n",
    ")\n",
    "\n",
    "# Plot the anomaly attribution scores with confidence intervals\n",
    "bar_plot(median_attributions, confidence_intervals, 'Anomaly attribution score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0611667-2d52-43e5-a89a-5068736ef097",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The bar chart above shows the anomaly attribution scores for the nodes associated with a product that failed the quality check due to dimensional verification. Positive values represent nodes that increased the likelihood of the sample being an anomaly, while negative values indicate the opposite. The confidence intervals reflect the uncertainty in the results, stemming from the fitted model parameters and algorithmic approximations. More details about the interpretation of the score can be found in the corresponding [research paper](https://proceedings.mlr.press/v162/budhathoki22a.html).\n",
    "\n",
    "Notably, `worker` and `machine` stand out as the primary factors influencing `quality`. This aligns with our synthetic data generation, where `worker = 1` and `machine = 1` were designed to be less precise in positioning and aligning materials and equipment. Interestingly, other factors, such as `position_alignment`, also seem to impact `quality`, as indicated by its positive contribution. However, this factor was not explicitly modeled (latent variable) in our causal graph and was instead included as a source of noise in the data generation process (see the method `generate_data` in the notebook `99_utils` for more information).\n",
    "\n",
    "If we observe numerous defective products with a similar root cause structure, we can confidently intervene by refining machine calibration protocols or introducing enhanced worker training programs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "889d6244-b98a-4b81-a8ce-fa2ae6c80996",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In the second example, we will see the root causes of a product that failed the **torque resistance check**. For this, we select one defective sample from the generated batch that did not pass the torque checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "beaae57c-145b-4812-aaea-ecf8fc17c4ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "defect_torque_checks = new_batch[new_batch['torque_checks'] == 1].iloc[0]\n",
    "display(defect_torque_checks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ec858c4-54d2-406a-b359-b88771319b71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "median_attributions, confidence_intervals, = gcm.confidence_intervals(\n",
    "    gcm.fit_and_compute(\n",
    "        gcm.attribute_anomalies,\n",
    "        loaded_scm,\n",
    "        bootstrap_training_data=train,\n",
    "        target_node='quality',\n",
    "        anomaly_samples=pd.DataFrame([defect_torque_checks])\n",
    "        ),\n",
    "    num_bootstrap_resamples=10\n",
    "    )\n",
    "    \n",
    "bar_plot(median_attributions, confidence_intervals, 'Anomaly attribution score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15d13a95-e4f2-4592-b40d-13693b6f56d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This time, `chamber_humidity` emerges as the primary factor influencing `quality`. This aligns with our synthetic data generation logic, where higher `chamber_humidity` was designed to lower interface `temperature` due to condensation and evaporation effects. Examining the values for this sample—`chamber_humidity` (0.79) and `temperature` (899)—reveals significant deviations from their expected standard values (0.5 and 1250, respectively; see the `generate_data` function in the `99_utils` notebook for more details). These deviations ultimately led to the torque resistance failure, as the lower interface `temperature` resulted in weak bonds.\n",
    "\n",
    "A potential countermeasure for this issue is to install equipment designed to minimize humidity fluctuations within the processing chamber."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84d1d7e0-8846-48dc-afb7-eb0254d0c109",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Finally, we will see the root causes of a product that failed the **visual inspection**. For this, we select one defective sample from the generated batch that did not pass the visual inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9151759-3e5e-416c-9a20-99a211340053",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "defect_visual_inspection = new_batch[new_batch['visual_inspection'] == 1].iloc[0]\n",
    "display(defect_visual_inspection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57e77a55-9c64-45d0-8a87-5617f9d2a949",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "median_attributions, confidence_intervals, = gcm.confidence_intervals(\n",
    "    gcm.fit_and_compute(\n",
    "        gcm.attribute_anomalies,\n",
    "        loaded_scm,\n",
    "        bootstrap_training_data=train,\n",
    "        target_node='quality',\n",
    "        anomaly_samples=pd.DataFrame([defect_visual_inspection])\n",
    "        ),\n",
    "    num_bootstrap_resamples=10\n",
    "    )\n",
    "    \n",
    "bar_plot(median_attributions, confidence_intervals, 'Anomaly attribution score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44c4a148-c3c9-4f6b-b73f-9694883dae0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Our final analysis reveals that `chamber_humidity` and `chamber_temperature` are the primary factors influencing `quality`. Interestingly, unlike the previous example, this anomaly is attributed to `chamber_humidity` being significantly ***lower*** than the standard. Combined with the elevated `chamber_temperature`, this resulted in a higher interface `temperature`, creating conditions for undesirable outcomes, such as excessive welding spatters, which ultimately caused the visual inspection checks to fail. Additionally, there appears to be an unmodeled factor impacting `temperature`, increasing the likelihood of this sample being classified as an anomaly—something not captured in our causal graph. To prevent similar defects from occurring, we could issue an alert and prompt manual workers to take preventive actions when `chamber_humidity` falls below and `chamber_temperature` rises above certain thresholds simultaneously.\n",
    "\n",
    "Causal AI provides transparent identification of attribute combinations contributing to undesired outcomes at the sample level, offering deeper insights compared to traditional correlation-based machine learning approaches.\n",
    "\n",
    "***Note: Given the stochastic nature of sampling, the displayed anomaly distribution might differ slightly from what is discussed here. We encourage users to experiment with the `confidence_intervals` feature using a variety of samples.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "274d68e3-a99d-4898-80ca-9235eb1425f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. What caused the quality drop in a batch of products?\n",
    "\n",
    "In the previous section, we focused on anomaly attribution for a single observation flagged as defective. However, such defects could result from an unfortunate combination of statistical outliers—rare events that occur probabilistically but occasionally. For these rare instances, there may not be many meaningful interventions we can take.\n",
    "\n",
    "On the other hand, if we observe a drop in quality across a batch of products rather than just a single sample, it is more likely that a shift in the underlying data generation process has caused the quality deterioration. In this section, we will explore a shift in quality across batches of products. To illustrate this, we will simulate a scenario where a new batch of products exhibits a significantly higher defect rate. We will generate a new batch using the funtion `generate_data` in the notebook `99_utils`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54bbc679-d080-4810-9501-686eee5daa49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test = generate_data(catalog, schema, 100, p_worker=0.25, train=False)\n",
    "test['quality'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db4d6d89-d1c3-44a3-97f5-968c5aa57905",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The defect rate has indeed increased from 0.075 in our training dataset to 0.17. Here’s what happened behind the scenes with the new batch: `worker = 0`, who previously handled 75% of the products, is on leave, and `worker = 1` has taken over her role. In this new batch, `worker = 1` processes 75% of the products as a manual operator, while `worker = 0` handles only 25%. We can simulate this distributional drift using the parameter `p_worker` in the function `generate_data`.\n",
    "\n",
    "Let’s now see if DoWhy can accurately identify the root cause of this change in the batch defect rate. We will apply the [distribution change method](https://proceedings.mlr.press/v130/budhathoki21a.html) to identify the part in the system that has changed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c2751d6-4486-41af-b61e-22f190f61de8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "median_attributions, confidence_intervals = gcm.confidence_intervals(\n",
    "    lambda: gcm.distribution_change(loaded_scm,\n",
    "                                    train,\n",
    "                                    test,\n",
    "                                    target_node='quality',\n",
    "                                    # Here, we are intersted in explaining the differences in the mean.\n",
    "                                    difference_estimation_func=lambda x, y: np.mean(y) - np.mean(x)) \n",
    ")\n",
    "\n",
    "bar_plot(median_attributions, confidence_intervals, 'Change attribution in defect rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f528ceed-de54-46e8-a952-bb32b2494ae8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The distribution change method accurately identifies the root cause of the increased defect rate, clearly indicating that the shift in `worker` has significantly contributed to this change. Since this is likely not a statistical fluke (given the aggregation over 100 samples), we can now confidently propose an intervention plan to enhance operational efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af5d92f1-18f3-47aa-9117-f09441d71368",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Wrap up\n",
    "\n",
    "In this notebook, we addressed the ultimate goal of our use case: uncovering the true root causes behind quality drops. We conducted root cause analysis on individual products flagged as defective for various reasons. We aslo analyzed a batch of products with a higher defect rate compared to the training dataset, identifying shifts in the underlying data generation processes. By pinpointing the key factors contributing to defects, we pave the way for targeted interventions that can enhance overall product quality and operational efficiency.\n",
    "\n",
    "In the next notebook, we will deploy the fitted graph to Databricks Model Serving and explore how to enable real-time causal analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c18f6101-4f34-427e-87d3-567239133987",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "© 2025 Databricks, Inc. All rights reserved. The source in this notebook is provided subject to the Databricks License. All included or referenced third party libraries are subject to the licenses set forth below.\n",
    "\n",
    "| library                                | description             | license    | source                                              |\n",
    "|----------------------------------------|-------------------------|------------|-----------------------------------------------------|\n",
    "| Graphviz | An open source graph visualization software | Common Public License Version 1.0 | https://graphviz.org/download/\n",
    "| pygraphviz | A Python interface to the Graphviz graph layout and visualization package | BSD | https://pypi.org/project/pygraphviz/\n",
    "| networkx | A Python package for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks. | BSD | https://pypi.org/project/networkx/\n",
    "| dowhy | A Python library for causal inference that supports explicit modeling and testing of causal assumptions | MIT | https://pypi.org/project/dowhy/\n",
    "| causal-learn | A python package for causal discovery that implements both classical and state-of-the-art causal discovery algorithms, which is a Python translation and extension of Tetrad. | MIT | https://pypi.org/project/causal-learn/\n",
    "| lime | Local Interpretable Model-Agnostic Explanations for machine learning classifiers | BSD | https://pypi.org/project/lime/\n",
    "| shap | A unified approach to explain the output of any machine learning model | MIT | https://pypi.org/project/shap/"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "03_offline_analysis",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
